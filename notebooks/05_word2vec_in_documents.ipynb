{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec in full documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim.models.word2vec as w2v\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Split corpus into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "data_tr = sklearn.datasets.twenty_newsgroups.fetch_20newsgroups(subset=\"train\")#[\"data\"]\n",
    "data_te = sklearn.datasets.twenty_newsgroups.fetch_20newsgroups(subset=\"test\")#[\"data\"]\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    clean = clean.lower()\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in data_tr[\"data\"] + data_te[\"data\"]:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 5,743,602 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "\n",
    "### Train word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "num_epochs   = 10\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 0\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 5\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "seed = 1\n",
    "\n",
    "#optional Training algorithm: 1 for skip-gram; otherwise CBOW\n",
    "sg = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = w2v.Word2Vec(\n",
    "    sg=sg,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vec.build_vocab(sentences, keep_raw_vocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2VecVocab at 0x1a3fd33c18>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115065"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.vocabulary.raw_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_examples = len(sentences)\n",
    "total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 25s, sys: 2.35 s, total: 11min 27s\n",
      "Wall time: 1min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46280943, 57436020)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "word2vec.train(sentences,\n",
    "               epochs = num_epochs,\n",
    "               total_examples=total_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"../saved_models/\" + \"w2v_\" + str(num_features) +\"features_10epochs\"\n",
    "modelname  = \"word2vec_\" + str(num_features) +\"features\" + str(num_epochs)+\"epochs.w2v\"\n",
    "\n",
    "if not os.path.exists(foldername):\n",
    "    os.makedirs(foldername)\n",
    "    word2vec.save(os.path.join(foldername, modelname))\n",
    "else:\n",
    "    print(\"folder {} already exists\".format(foldername))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(word2vec.wv.vocab)\n",
    "vectors = list(word2vec.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115065"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting from a Word2vec averaged representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "data_tr = sklearn.datasets.twenty_newsgroups.fetch_20newsgroups(subset=\"train\")\n",
    "data_te = sklearn.datasets.twenty_newsgroups.fetch_20newsgroups(subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "word2vec.wv.get_vector(\"house\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_vec(sentence, word2vec):\n",
    "    word_list    = sentence_to_wordlist(sentence)\n",
    "    word_vectors = []\n",
    "    for w in word_list:\n",
    "        word_vectors.append(word2vec.wv.get_vector(w))\n",
    "\n",
    "    return np.mean(word_vectors,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300,), 11314)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = doc_to_vec(data_tr[\"data\"][0], word2vec)\n",
    "vec.shape, len(data_tr[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.zeros((len(data_tr[\"data\"]), 300))\n",
    "y_tr = data_tr[\"target\"]\n",
    "n_samples = X_tr.shape[0]\n",
    "\n",
    "for i in range(n_samples):\n",
    "    X_tr[i,:] = doc_to_vec(data_tr[\"data\"][i], word2vec)\n",
    "\n",
    "X_te = np.zeros((len(data_te[\"data\"]), 300))\n",
    "y_te = data_te[\"target\"]\n",
    "n_samples = X_te.shape[0]\n",
    "for i in range(n_samples):\n",
    "    X_te[i,:] = doc_to_vec(data_te[\"data\"][i], word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with a Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidbuchaca1/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/davidbuchaca1/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 300), (7532, 300))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy for movie plots0.9394555418066113\n",
      "Train F1 score for movie plots: 0.9392440455365525\n",
      "Test accuracy for movie plots0.7365905469994689\n",
      "Test F1 score for movie plots: 0.7379816450290826\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_tr)\n",
    "print('Train accuracy for movie plots: {}'.format(accuracy_score(y_tr, y_pred)))\n",
    "print('Train F1 score for movie plots: {}'.format(f1_score(y_tr, y_pred, average='weighted')))\n",
    "\n",
    "y_pred = logreg.predict(X_te)\n",
    "print('Test accuracy for movie plots: {}'.format(accuracy_score(y_te, y_pred)))\n",
    "print('Test F1 score for movie plots: {}'.format(f1_score(y_te, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 300), (7532, 300))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidbuchaca1/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=[300], learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import neural_network\n",
    "\n",
    "clf = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=[100])\n",
    "clf.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy for movie plots0.9795828177479229\n",
      "Train F1 score for movie plots: 0.9795884693543918\n",
      "Test accuracy for movie plots0.7895645246946362\n",
      "Test F1 score for movie plots: 0.7887611372446388\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_tr)\n",
    "print('Train accuracy for movie plots: {}'.format(accuracy_score(y_tr, y_pred)))\n",
    "print('Train F1 score for movie plots: {}'.format(f1_score(y_tr, y_pred, average='weighted')))\n",
    "\n",
    "y_pred = clf.predict(X_te)\n",
    "print('Test accuracy for movie plots: {}'.format(accuracy_score(y_te, y_pred)))\n",
    "print('Test F1 score for movie plots: {}'.format(f1_score(y_te, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting from a count matrix appended with word2vec features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'w2v_5000features_10epochs',\n",
       " 'w2v_2000features_10epochs',\n",
       " 'w2v_600features_10epochs',\n",
       " 'w2v_300features_10epochs']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../saved_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word2vec_300features10epochs.w2v.wv.vectors.npy',\n",
       " 'word2vec_300features10epochs.w2v',\n",
       " 'word2vec_300features10epochs.w2v.trainables.syn1neg.npy']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../saved_models/w2v_300features_10epochs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"../saved_models/w2v_300features_10epochs\"\n",
    "modelname  = \"word2vec_300features10epochs.w2v\"\n",
    "word2vec   = w2v.Word2Vec.load(os.path.join(foldername, modelname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 7532)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = sklearn.datasets.twenty_newsgroups.fetch_20newsgroups(subset=\"train\")[\"data\"]\n",
    "X_test  = sklearn.datasets.twenty_newsgroups.fetch_20newsgroups(subset=\"test\")[\"data\"]\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "\n",
    "X_train = count_vectorizer.fit_transform(X_train)\n",
    "X_test  = count_vectorizer.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "import scipy\n",
    "\n",
    "X_tr_extended = scipy.sparse.hstack((X_tr, X_train))\n",
    "X_te_extended = scipy.sparse.hstack((X_te, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidbuchaca1/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/davidbuchaca1/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy for movie plots0.999734841788934\n",
      "Train F1 score for movie plots: 0.9997348419755528\n",
      "Test accuracy for movie plots0.8112055231014339\n",
      "Test F1 score for movie plots: 0.8103034177382152\n",
      "CPU times: user 1min 7s, sys: 378 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_logit_ext = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "clf_logit_ext.fit(X_tr_extended, y_tr)\n",
    "y_pred = clf_logit_ext.predict(X_tr_extended)\n",
    "print('Train accuracy for movie plots: {}'.format(accuracy_score(y_tr, y_pred)))\n",
    "print('Train F1 score for movie plots: {}'.format(f1_score(y_tr, y_pred,\n",
    "                                                           average='weighted')))\n",
    "\n",
    "y_pred = clf_logit_ext.predict(X_te_extended)\n",
    "print('Test accuracy for movie plots: {}'.format(accuracy_score(y_te, y_pred)))\n",
    "print('Test F1 score for movie plots: {}'.format(f1_score(y_te, y_pred,\n",
    "                                                          average='weighted')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy for movie plots0.999734841788934\n",
      "Train F1 score for movie plots: 0.9997348426792321\n",
      "Test accuracy for movie plots0.831651619755709\n",
      "Test F1 score for movie plots: 0.8317547230992746\n",
      "CPU times: user 23min 8s, sys: 3min 33s, total: 26min 41s\n",
      "Wall time: 14min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_mlp_ext = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=[100],\n",
    "                                                   validation_fraction=0.1 )\n",
    "clf_mlp_ext.fit(X_tr_extended, y_tr)\n",
    "y_pred = clf_mlp_ext.predict(X_tr_extended)\n",
    "print('Train accuracy for movie plots%s' % accuracy_score(y_tr, y_pred))\n",
    "print('Train F1 score for movie plots: {}'.format(f1_score(y_tr, y_pred, \n",
    "                                                           average='weighted')))\n",
    "\n",
    "y_pred = clf_mlp_ext.predict(X_te_extended)\n",
    "print('Test accuracy for movie plots: {}'.format(accuracy_score(y_te, y_pred)))\n",
    "print('Test F1 score for movie plots: {}'.format(f1_score(y_te, y_pred,\n",
    "                                                          average='weighted')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending word2vec and using ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"../saved_models/\" + \"w2v_\" + str(num_features) +\"features_10epochs\"\n",
    "modelname  = \"word2vec_\" + str(num_features) +\"features\" + str(num_epochs)+\"epochs.w2v\"\n",
    "\n",
    "word2vec = w2v.Word2Vec.load(os.path.join(foldername, modelname))\n",
    "n_features = word2vec.trainables.layer1_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.zeros((len(data_tr[\"data\"]), n_features))\n",
    "y_tr = data_tr[\"target\"]\n",
    "n_samples = X_tr.shape[0]\n",
    "\n",
    "for i in range(n_samples):\n",
    "    X_tr[i,:] = doc_to_vec(data_tr[\"data\"][i], word2vec)\n",
    "\n",
    "X_te = np.zeros((len(data_te[\"data\"]), n_features))\n",
    "y_te = data_te[\"target\"]\n",
    "n_samples = X_te.shape[0]\n",
    "for i in range(n_samples):\n",
    "    X_te[i,:] = doc_to_vec(data_te[\"data\"][i], word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "X_train = sklearn.datasets.twenty_newsgroups.fetch_20newsgroups(subset=\"train\")[\"data\"]\n",
    "X_test  = sklearn.datasets.twenty_newsgroups.fetch_20newsgroups(subset=\"test\")[\"data\"]\n",
    "\n",
    "X_train = count_vectorizer.fit_transform(X_train)\n",
    "X_test  = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_extended = scipy.sparse.hstack((X_tr, X_train))\n",
    "X_te_extended = scipy.sparse.hstack((X_te, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 0 ns, total: 12 µs\n",
      "Wall time: 15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_mlp_ext_ngram = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=[200],\n",
    "                                                         validation_fraction=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_mlp_ext_ngram.fit(X_tr_extended, y_tr)\n",
    "\n",
    "y_pred = clf_mlp_ext_ngram.predict(X_tr_extended)\n",
    "print('Train accuracy for movie plots: {}'.format(accuracy_score(y_tr, y_pred)))\n",
    "print('Train F1 score for movie plots: {}'.format(f1_score(y_tr, y_pred,\n",
    "                                                           average='weighted')))\n",
    "\n",
    "y_pred = clf_mlp_ext_ngram.predict(X_te_extended)\n",
    "print('Test accuracy for movie plots: {}'.format(accuracy_score(y_te, y_pred)))\n",
    "print('Test F1 score for movie plots: {}'.format(f1_score(y_te, y_pred,\n",
    "                                                          average='weighted')))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More  document features\n",
    "\n",
    "\n",
    "We will denote by\n",
    "\n",
    "- $W= \\{w_1, \\dots, w_D\\}$ the set of words used to make the representations.\n",
    "- $X$ our corpus of documents.\n",
    "- $X_w$ the set of documents that contain word $w$. \n",
    "\n",
    "### Bag of words vector  (or `tf` vector)\n",
    "\n",
    "\n",
    "- The bag of words representation for a document $x$ given a vocabulary $W$, or the term frequency vector **$\\text{tf}(X;W)$** is defined as \n",
    "\n",
    "$$\n",
    "\\text{tf}(x;W) = \\left( \\#\\{w_1| w_1 \\in x\\}, \\dots, \\#\\{w_D| w_D \\in x\\})\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Term frequency Inverse Document frequency ( `tf * idf`)\n",
    "\n",
    "The objective of tf-idf representation is to emphasize the most relevant words of the documents. We want to emphasize:\n",
    "\n",
    "- Words that appear **frequently in the document**: term frequency \n",
    "- Words that appear **rarely in the corpus**: inverse document frequency\n",
    "\n",
    "#### Definition of the feature vectors\n",
    "\n",
    "\n",
    "- The **$\\text{tf}(X;W)$** vector for a document $x$ is defined as \n",
    "\n",
    "$$\n",
    "\\text{tf}(x;W) = \\left( \\#\\{w_1| w_1 \\in x\\}, \\dots, \\#\\{w_D| w_D \\in x\\})\\right)\n",
    "$$\n",
    "\n",
    "- The **$\\text{idf}(W; X)$** vector is defined as \n",
    "\n",
    "**$$\\text{idf}(W; X) = \\left( \\text{idf}(w_1; X), \\dots, \\text{idf}(w_D; X)\\right)$$** \n",
    "   \n",
    "$\\,\\,\\,\\,\\,\\,\\,$ A component of the feature for word $w \\in W$ in the corpus $X$ is defined as \n",
    "\n",
    "$$\n",
    "\\text{idf}(w, X) = log\\left(\\frac{|X|}{1+|X_{w}|}\\right)\n",
    "$$\n",
    "\n",
    "$\\,\\,\\,\\,\\,\\,\\,$Which simply means the full vector is \n",
    "$$\n",
    "\\text{idf}(w, X) = \\left( log\\left(\\frac{|X|}{1+|X_{w_1}|}\\right), \\dots, log\\left(\\frac{|X|}{1+|X_{w_D}|}\\right) \\right)\n",
    "$$\n",
    "\n",
    "- The tfidf vector for a document $x$ will be: $tf(x; X) * idf(X)$\n",
    "\n",
    "#### Observations\n",
    "\n",
    "- If a word appears in a few documents the idf vector will increase its weight.\n",
    "\n",
    "- If a word appears in a lots of documents documents the idf vector will decrease its weight.\n",
    "\n",
    "#### `sklearn.feature_extraction.text.TfidfVectorizer`\n",
    "\n",
    "- Notice that the implementation in sklearn already prevents zero divisions by default. This happens if `smooth_idf=True`.\n",
    "\n",
    "- By default the tfidf will only use words since `ngram_range=(1, 1)`. But this can be changed to allow n-grams in the feature vector components.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let us assume we have a corpus with one milion documents\n",
    "\n",
    "- Consider a word appearping in 100 documents:\n",
    "\n",
    "$$\\log\\left(\\frac{1000.000}{1 + 100} \\right) = 9.200$$\n",
    "\n",
    "- Consider a word appearing in 100.000 documents\n",
    "\n",
    "$$\\log\\left(\\frac{1000.000}{1 + 100.000} \\right) = 2.197$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def build_vocabulary(corpus, splitter):\n",
    "    \"\"\"\n",
    "    This function has to return X_w, a dict containing for each key, how\n",
    "    many documents having that key are in our corpus.\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    X_w = dict()\n",
    "    \n",
    "    for document in corpus:\n",
    "        words      = set(splitter.findall(document.lower()))\n",
    "        # fill up vocabulary \n",
    "        \n",
    "        # fill up X_w\n",
    "        \n",
    "    return vocabulary, X_w\n",
    "\n",
    "def term_frequency(document, word_to_ind, splitter, \n",
    "                   normalize=True, word_inds=False):\n",
    "    \n",
    "    words = splitter.findall(document.lower())\n",
    "    n_features = len(word_to_ind)\n",
    "    tf = sp.sparse.lil_matrix( (1, n_features), dtype=float)\n",
    "    \n",
    "    word_indices = []\n",
    "    for w in words:\n",
    "        ## fill up word_indices\n",
    "        \n",
    "        ## fill up tf\n",
    "        \n",
    "        \n",
    "    if word_inds:\n",
    "        if normalize:\n",
    "            return tf.multiply(1/sp.sparse.linalg.norm(tf))\n",
    "        else:\n",
    "            return tf\n",
    "    else:\n",
    "        if normalize:\n",
    "            return tf.multiply(1/sp.sparse.linalg.norm(tf))\n",
    "        else:\n",
    "            return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.9 s, sys: 96 ms, total: 25 s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "splitter = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "%time vocabulary, X_w = build_vocabulary(newsgroups_train.data, splitter)\n",
    "\n",
    "word_to_ind = {v:i for i,v in enumerate(vocabulary)}\n",
    "ind_to_word = {v:k for k,v in word_to_ind.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.45 ms, sys: 300 µs, total: 1.75 ms\n",
      "Wall time: 1.44 ms\n"
     ]
    }
   ],
   "source": [
    "%time tf = term_frequency(newsgroups_train.data[0],\\\n",
    "                          word_to_ind, splitter, word_inds=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that the term frequency is OK, compare with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 s, sys: 43.4 ms, total: 2.12 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "tfidf_sk = sklearn.feature_extraction.text.TfidfVectorizer(use_idf=False,\n",
    "                                                           smooth_idf=False, \n",
    "                                                           sublinear_tf=False)\n",
    "\n",
    "%time tfidf_sk.fit(newsgroups_train.data)\n",
    "\n",
    "inverse_vocabulary_ = {v: k for k, v in tfidf_sk.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 321 µs, sys: 6 µs, total: 327 µs\n",
      "Wall time: 327 µs\n"
     ]
    }
   ],
   "source": [
    "%time x_sk = tfidf_sk.transform([newsgroups_train.data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(tf.sum(), x_sk.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_x_own = [ind_to_word[k] for k in tf.nonzero()[1]]\n",
    "words_x_sk = [inverse_vocabulary_[k] for k in x_sk.nonzero()[1]]\n",
    "set(words_x_own) == set(words_x_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tfidf and compare with sklearn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(X_w, word_to_ind, n_documents):\n",
    "\n",
    "    n_features = len(word_to_ind)\n",
    "    #idf = sp.sparse.csr_matrix( (1, n_features), dtype=float)\n",
    "    idf = np.zeros([1, n_features])\n",
    "    \n",
    "    for w in X_w:\n",
    "        # fill up idf\n",
    "        pass\n",
    "    \n",
    "    #idf = idf + 1    \n",
    "    return sp.sparse.csr_matrix(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 178 ms, sys: 1.41 ms, total: 179 ms\n",
      "Wall time: 178 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# lil_matrix is more efficient.\n",
    "tf = term_frequency(newsgroups_train.data[0], word_to_ind,\\\n",
    "                    splitter, normalize=False,word_inds=False)\n",
    "\n",
    "idf = compute_idf(X_w,word_to_ind, len(newsgroups_train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.640737377507692, 1.0)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf.max(), idf.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_documents = len(X_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = tf.multiply(idf)\n",
    "tfidf = tfidf/sp.sparse.linalg.norm(tfidf)\n",
    "sp.sparse.linalg.norm(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(newsgroups_train.data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_sklearn = tfidf_vectorizer.transform(newsgroups_train.data[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.data.dtype, tfidf_sklearn.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.697815233022508 7.697815233022508\n",
      "\n",
      "sklearn tfidf and our tfidf are the same: True\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.sum(), tfidf_sklearn.sum())\n",
    "print(\"\\nsklearn tfidf and our tfidf are the same:\",\n",
    "      np.isclose(tfidf_sklearn.sum(),tfidf.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.read_csv('../data/people_wiki.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.loc[0:40000].to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Digby_Morrell&gt;</td>\n",
       "      <td>Digby Morrell</td>\n",
       "      <td>digby morrell born 10 october 1979 is a former...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Alfred_J._Lewy&gt;</td>\n",
       "      <td>Alfred J. Lewy</td>\n",
       "      <td>alfred j lewy aka sandy lewy graduated from un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Harpdog_Brown&gt;</td>\n",
       "      <td>Harpdog Brown</td>\n",
       "      <td>harpdog brown is a singer and harmonica player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Franz_Rottensteiner&gt;</td>\n",
       "      <td>Franz Rottensteiner</td>\n",
       "      <td>franz rottensteiner born in waidmannsfeld lowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/G-Enka&gt;</td>\n",
       "      <td>G-Enka</td>\n",
       "      <td>henry krvits born 30 december 1974 in tallinn ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URI                 name  \\\n",
       "0        <http://dbpedia.org/resource/Digby_Morrell>        Digby Morrell   \n",
       "1       <http://dbpedia.org/resource/Alfred_J._Lewy>       Alfred J. Lewy   \n",
       "2        <http://dbpedia.org/resource/Harpdog_Brown>        Harpdog Brown   \n",
       "3  <http://dbpedia.org/resource/Franz_Rottensteiner>  Franz Rottensteiner   \n",
       "4               <http://dbpedia.org/resource/G-Enka>               G-Enka   \n",
       "\n",
       "                                                text  \n",
       "0  digby morrell born 10 october 1979 is a former...  \n",
       "1  alfred j lewy aka sandy lewy graduated from un...  \n",
       "2  harpdog brown is a singer and harmonica player...  \n",
       "3  franz rottensteiner born in waidmannsfeld lowe...  \n",
       "4  henry krvits born 30 december 1974 in tallinn ...  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URI     object\n",
       "name    object\n",
       "text    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'franz rottensteiner born in waidmannsfeld lower austria austria on 18 january 1942 is an austrian publisher and critic in the fields of science fiction and the fantasticrottensteiner studied journalism english and history at the university of vienna receiving his doctorate in 1969 he served about fifteen years as librarian and editor at the sterreichisches institut fr bauforschung in vienna in addition he produced a number of translations into german of leading sf authors including herbert w franke stanislaw lem philip k dick kobo abe cordwainer smith brian w aldiss and the strugatski brothersin 1973 his new york anthology view from another shore of european science fiction introduced a number of continental authors to the englishreading public some of the authors in the work are stanislaw lem josef nesvadba gerard klein and jeanpierre andrevonthe year 1975 saw the start of his series die phantastischen romane for seven years it republished works of both lesser and betterknown writers as well as new ones ending with a total of 28 volumes in the years 19791985 he brought out trnaslations of h g wellss works in an eighteen volumes seriesrottensteiner provoked some controversy with his negative assessment of american science fiction what matters is the highest achievements and there the us has yet to produce a figure comparable to hg wells olaf stapledon karel apek or stanisaw lem rottensteiner describedroger zelazny barry n malzberg and robert silverberg as producing travesties of fiction and stated asimov is a typical nonwriter and heinlein and andersonare just banal however rottensteiner praised philip k dick listing him as one of the greatest sf writers from 1980 through 1998 he was advisor for suhrkamp verlags phantastische bibliothek which brought out some three hundred books in all he has edited about fifty anthologies produced two illustrated books the science fiction book 1975 und the fantasy book 1978 as well as working on numerous reference works on science fiction his close association with and promotion of lem until 1995 was a factor in the recognition of the latter in the united statesrottensteiner has been the editor of quarber merkur the leading german language critical journal of science fiction since 1963 in 2004 on the occasion of the hundredth number of this journal he was awarded a special kurdlawitzpreis'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people[\"text\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama = people[people['name'] == 'Barack Obama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35817    barack hussein obama ii brk husen bm born augu...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people[people['name'] == 'Barack Obama'][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(df, boolean_series):\n",
    "    row_df = df[boolean_series].text\n",
    "    return df.loc[row_df.index[0]].text\n",
    "\n",
    "def get_text_given_name(df, name):\n",
    "    row_df = df[df[\"name\"] == name]\n",
    "    return df.loc[row_df.index[0]].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'barack hussein obama ii brk husen bm born august 4 1961 is the 44th and current president of the united states and the first african american to hold the office born in honolulu hawaii obama is a graduate of columbia university and harvard law school where he served as president of the harvard law review he was a community organizer in chicago before earning his law degree he worked as a civil rights attorney and taught constitutional law at the university of chicago law school from 1992 to 2004 he served three terms representing the 13th district in the illinois senate from 1997 to 2004 running unsuccessfully for the united states house of representatives in 2000in 2004 obama received national attention during his campaign to represent illinois in the united states senate with his victory in the march democratic party primary his keynote address at the democratic national convention in july and his election to the senate in november he began his presidential campaign in 2007 and after a close primary campaign against hillary rodham clinton in 2008 he won sufficient delegates in the democratic party primaries to receive the presidential nomination he then defeated republican nominee john mccain in the general election and was inaugurated as president on january 20 2009 nine months after his election obama was named the 2009 nobel peace prize laureateduring his first two years in office obama signed into law economic stimulus legislation in response to the great recession in the form of the american recovery and reinvestment act of 2009 and the tax relief unemployment insurance reauthorization and job creation act of 2010 other major domestic initiatives in his first term included the patient protection and affordable care act often referred to as obamacare the doddfrank wall street reform and consumer protection act and the dont ask dont tell repeal act of 2010 in foreign policy obama ended us military involvement in the iraq war increased us troop levels in afghanistan signed the new start arms control treaty with russia ordered us military involvement in libya and ordered the military operation that resulted in the death of osama bin laden in january 2011 the republicans regained control of the house of representatives as the democratic party lost a total of 63 seats and after a lengthy debate over federal spending and whether or not to raise the nations debt limit obama signed the budget control act of 2011 and the american taxpayer relief act of 2012obama was reelected president in november 2012 defeating republican nominee mitt romney and was sworn in for a second term on january 20 2013 during his second term obama has promoted domestic policies related to gun control in response to the sandy hook elementary school shooting and has called for full equality for lgbt americans while his administration has filed briefs which urged the supreme court to strike down the defense of marriage act of 1996 and californias proposition 8 as unconstitutional in foreign policy obama ordered us military involvement in iraq in response to gains made by the islamic state in iraq after the 2011 withdrawal from iraq continued the process of ending us combat operations in afghanistan and has sought to normalize us relations with cuba'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_given_name(people, \"Barack Obama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_vec, emma_vec = tfidf.fit_transform([get_text_given_name(people, \"Barack Obama\"), \n",
    "                                           get_text_given_name(people, \"Emma Watson\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 s, sys: 233 ms, total: 12.3 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_tfidf = tfidf.fit_transform(people[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scp\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "brad_pitt_tfidf = tfidf.transform([get_text_given_name(people, \"Brad Pitt\")])\n",
    "angelina_tfidf  = tfidf.transform([get_text_given_name(people, \"Angelina Jolie\")])\n",
    "obama_tfidf     = tfidf.transform([get_text_given_name(people, \"Barack Obama\")])\n",
    "bill_tfidf      = tfidf.transform([get_text_given_name(people, \"Bill Clinton\")])\n",
    "emma_tfidf      = tfidf.transform([get_text_given_name(people, \"Emma Watson\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_k_names(tfidf_vec, X_tfidf, k=10):\n",
    "    aux = np.argsort(cosine_similarity(tfidf_vec, X_tfidf))\n",
    "    return people[\"name\"][aux[0][-k-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47793           Miranda Richardson\n",
       "40216               Carolyn Watson\n",
       "11666                   Jane Fonda\n",
       "34756                 Maggie Smith\n",
       "35902            Natashia Williams\n",
       "8973                  John Granger\n",
       "17821                Emma Thompson\n",
       "46781                 Lilla Watson\n",
       "53752    Patrick Watson (producer)\n",
       "3115                  Stuart Craig\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_k_names(emma_tfidf, X_tfidf, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54452         George H. W. Bush\n",
       "35817              Barack Obama\n",
       "54499        Kris M. Balderston\n",
       "2092         Richard Blumenthal\n",
       "54062           James A. Joseph\n",
       "28447            George W. Bush\n",
       "52859                 Ann Lewis\n",
       "4096           Sheffield Nelson\n",
       "25658               Dick Morris\n",
       "57108    Hillary Rodham Clinton\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_k_names(bill_tfidf, X_tfidf, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24426             Brad Pitt\n",
       "51145          Kate Winslet\n",
       "21644          Jodie Foster\n",
       "44571        Candice Bergen\n",
       "16242          Meryl Streep\n",
       "54362    Konkona Sen Sharma\n",
       "44992        Julianne Moore\n",
       "34756          Maggie Smith\n",
       "57434           Glenn Close\n",
       "29009       Barbara Hershey\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_k_names(angelina_tfidf, X_tfidf, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_point(all_points, query_point, dist):\n",
    "    closest_point_    = None\n",
    "    closest_distance_ = np.inf\n",
    "    \n",
    "    for current_point in all_points:\n",
    "        current_distance = dist(query_point, current_point)\n",
    "        \n",
    "        if  current_distance < closest_distance_:\n",
    "            closest_distance_ = current_distance\n",
    "            closest_point_    = current_point\n",
    "            \n",
    "    return closest_point_, closest_distance_\n",
    "\n",
    "def dist(x,y):\n",
    "    return np.sqrt(np.linalg.norm((x-y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: [1 3]\n",
      "Closest to query: [0 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a4c1d25f8>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a50270a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X = np.array([[1,1],[0,3],[4,3],[0,0],[1,5],[6,1],[-2,1],[4,4],[2,1],[-1,0]])\n",
    "x_components = [x[0] for x in X]\n",
    "y_components = [x[1] for x in X]\n",
    "query = np.array([1,3])\n",
    "\n",
    "closest_to_query, _ = closest_point(X, query, dist)\n",
    "print(\"Query: {}\\nClosest to query: {}\".format(query,closest_to_query))\n",
    "plt.scatter(x_components, y_components, color=\"black\")\n",
    "plt.scatter(query[0], query[1], color=\"red\")\n",
    "plt.scatter(closest_to_query[0], closest_to_query[1], color=\"blue\", linewidths=1, marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding closest point in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import neighbors\n",
    "\n",
    "n_features = 20\n",
    "X = np.random.rand(10_000_000,n_features).astype(np.float32)\n",
    "x = np.random.rand(1,n_features).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 441 ms, sys: 173 ms, total: 614 ms\n",
      "Wall time: 614 ms\n",
      "CPU times: user 430 ms, sys: 176 ms, total: 606 ms\n",
      "Wall time: 606 ms\n",
      "\n",
      "closest row from x is 6692578\n"
     ]
    }
   ],
   "source": [
    "%time distances =  np.mean((X-x)**2,1)\n",
    "#One second is too much\n",
    "%time closest = np.argmin(np.mean((X-x)**2,1))\n",
    "print(\"\\nclosest row from x is {}\".format(closest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02051361"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances[closest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6692578, 6444021, 5730515, ..., 1365964, 4872855, 9615531])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "closest = np.argmin(np.mean((X-x)**2,1))\n",
    "e = time.time()\n",
    "numpy_time = abs(s-e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom function for closest point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 434 ms, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "closest_point_, closest_distance_ = closest_point(X, x, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Kd tree\n",
    "\n",
    "KD-trees are an efficient structure for efficiently representing our data. KD-trees provide an organization of our documents in terms of a certain partitioning of our space. The organization is based on recursively partitioning points into axis, defining \"boxes\".\n",
    "\n",
    "The KD-tree structure is based on making aligned cuts and maintaining lists of points that fall into each one of these different bins. This structure allows us  efficiently prune our search space so that we do not have to visit every single data point, for every query, necessarily. Sometimes we will have to do it but hopefully, in many cases, we will not have to do it.\n",
    "\n",
    "\n",
    "#### Using KD-trees\n",
    "\n",
    "Let us see how KD-trees can aid in efficiently making NN search. Let us assume we are given a KD_tree and let us see how to ue it. Later on we will see how to build the tree.\n",
    "\n",
    "Given a query point $\\bf{x}$:\n",
    "\n",
    "- Traverse the tree until the query point is reached. That is, check all the conditions of the KD-tree for the query point until a leave is reached.\n",
    "    - Once the query point is found save the \"box\" where it is found.\n",
    "    \n",
    "    \n",
    "- Compute the distance between each neighbor in the box and the query point.\n",
    "\n",
    "\n",
    "- Record the smallest distance to the NN so far.\n",
    "\n",
    "\n",
    "- Backtrack and try other branch at each node visited.\n",
    "    - Use the distance bound and bounding box of each node to prune parts of the three that cannot include the nearest neighbor.\n",
    "         \n",
    "         That is, **if the smallest distance is less than the distance from the query point to the bounding box there is no need to compute the distance between any point in the bounding box to the query point**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.1 s, sys: 450 ms, total: 36.5 s\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tree = sklearn.neighbors.KDTree(X, leaf_size=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 285 ms, sys: 1.08 ms, total: 286 ms\n",
      "Wall time: 286 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "s = time.time()\n",
    "distance_to_closest, closest_kdtree = tree.query(x, k=1)\n",
    "e = time.time()\n",
    "kdtree_time = abs(s-e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "closest row from x is 6692578\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nclosest row from x is {}\".format(closest_kdtree[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy time: 0.6174180507659912\n",
      "Kdtree time: 0.2856941223144531\n"
     ]
    }
   ],
   "source": [
    "print(\"Numpy time:\", numpy_time)\n",
    "print(\"Kdtree time:\", kdtree_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building a kdtree with less leaves\n",
    "\n",
    "leaf_size : positive integer (default = 40)\n",
    "\n",
    "Number of points at which to switch to brute-force. Changing leaf_size will not affect the results of a query, but can significantly impact the speed of a query and the memory required to store the constructed tree.\n",
    "\n",
    "The amount of memory needed to store the tree scales as approximately n_samples / leaf_size. For a specified leaf_size, a leaf node is guaranteed to satisfy leaf_size <= n_points <= 2 * leaf_size, except in the case that n_samples < leaf_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.2 s, sys: 557 ms, total: 39.7 s\n",
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tree = sklearn.neighbors.KDTree(X, leaf_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.3 ms, sys: 901 µs, total: 81.2 ms\n",
      "Wall time: 80.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "s = time.time()\n",
    "distance_to_closest, closest_kdtree = tree.query(x, k=1)\n",
    "e = time.time()\n",
    "kdtree_time2 = abs(s-e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "closest row from x is 6692578\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nclosest row from x is {}\".format(closest_kdtree[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy time: 0.6174180507659912\n",
      "Kdtree(1000) time: 0.2856941223144531\n",
      "Kdtree(10)   time: 0.08020997047424316\n"
     ]
    }
   ],
   "source": [
    "print(\"Numpy time:\", numpy_time)\n",
    "print(\"Kdtree(1000) time:\", kdtree_time)\n",
    "print(\"Kdtree(10)   time:\", kdtree_time2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use also the kd tree to get the k closest items to our query vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 176 ms, sys: 1.18 ms, total: 177 ms\n",
      "Wall time: 176 ms\n"
     ]
    }
   ],
   "source": [
    "%time distances_to_closest, close_kdtree = tree.query(x,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
