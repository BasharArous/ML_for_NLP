{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# [2] NLP features from language\n",
    "\n",
    "## 1) Preprocessing words\n",
    "\n",
    "- Steaming \n",
    "- Lematization \n",
    "  \n",
    "\n",
    "## 2) Features for documents\n",
    "\n",
    "We can encode documents in lots of ways. Let us explore several strategies for converting raw strings to feature vectors representing whole documents.\n",
    "\n",
    "### 2.1) From docs to feature vectors: Make your own countvectorizer\n",
    "   - I)   No cleaning, no lemmatization\n",
    "   - II)  Yes cleaning, no lemmatization\n",
    "   - III) Yes cleaning, yes lemmatization\n",
    "\n",
    "### 2.2)  Ngram features with Sklearn vectorizer\n",
    "Use the standard vectorizer from sklaern\n",
    "\n",
    "   \n",
    "\n",
    "### 2.3) Feature selection\n",
    "\n",
    "We can have too many unreliable words (and/for combinations of words). Prunning some of this features might help us to generalize better. \n",
    " \n",
    "### 2.4) Crossvalidation (Exercise)\n",
    "\n",
    "We Crosvalidation with a pipeline to explore the different choices you make in the whole program. This can take some time. Use Random Crossvalidation to more efficiently explore the space if your hardware requirements are limited for the problem at hand.\n",
    "\n",
    "\n",
    "   \n",
    "### 2.5) Investigate hashing (Exercise)\n",
    "\n",
    "Use `feature_extraction.FeatureHasher` to generate your vectors. What performance do you get?\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Preprocessing words\n",
    "\n",
    "### 2.1.1) Steaming\n",
    "\n",
    "Steamming consist on removing the suffixes or prefixes used in word. The returned string from a lemmatizer might not be a valid word from the language.\n",
    "\n",
    "`Stem(saw) = saw`\n",
    "\n",
    "- Potter algorithm (1980), Lovins stemmer, Husk Stemmer (1990) are relevant algorithms to do steamming.\n",
    "\n",
    "\n",
    "We can use steamming adn lemmatization to reduce to a similar meaning different forms of similar words\n",
    "\n",
    "- Example:\n",
    "   \n",
    "    - `are`, `is` => `be`\n",
    "    - `man`, `man`, `man'`\n",
    "    \n",
    "Using this process we can trasnform `Dogs are Man's best friend` to `Dog be Man best friend`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter    = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                PorterStemmer       LancasterStemmer    \n",
      "\n",
      "dogs                dog                 dog                 \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n",
      "pass                pass                pass                \n",
      "passing             pass                pass                \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "passed              pass                pass                \n",
      "trouble             troubl              troubl              \n",
      "troubling           troubl              troubl              \n",
      "care                care                car                 \n",
      "believes            believ              believ              \n"
     ]
    }
   ],
   "source": [
    "words = [\"dogs\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\n",
    "         \"football\",\"pass\",\"passing\",\"friendship\", \"friends\", \"friendships\",\n",
    "         \"passed\",\"trouble\",\"troubling\",\"care\", \"believes\"]\n",
    "preprocess = [porter, lancaster]\n",
    "\n",
    "len_bin = 20\n",
    "col_formater = \"{0:len_bin}{1:len_bin}{2:len_bin}\".replace(\"len_bin\",str(len_bin))\n",
    "print(col_formater.format(\"Word\", porter.__class__.__name__, lancaster.__class__.__name__))\n",
    "print(\"\")\n",
    "for w in words:\n",
    "    print( col_formater.format(w, porter.stem(w), lancaster.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def stem(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    sentence_stemmed = []\n",
    "    for word in token_words:\n",
    "        sentence_stemmed.append(porter.stem(word))\n",
    "        sentence_stemmed.append(\" \")\n",
    "    return \"\".join(sentence_stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j.k. rowl wrote harri potter . she never expect the book to be famou . '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"J.K. Rowling wrote Harry Potter. She never expected the book to be famous.\"\n",
    "stem(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J.K. Rowling wrote Harry Potter.',\n",
       " 'She never expected the book to be famous.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J',\n",
       " 'K',\n",
       " ' Rowling wrote Harry Potter',\n",
       " ' She never expected the book to be famous',\n",
       " '']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Be carefull separating phrases\n",
    "s.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J.K.',\n",
       " 'Rowling',\n",
       " 'wrote',\n",
       " 'Harry',\n",
       " 'Potter',\n",
       " '.',\n",
       " 'She',\n",
       " 'never',\n",
       " 'expected',\n",
       " 'the',\n",
       " 'book',\n",
       " 'to',\n",
       " 'be',\n",
       " 'famous',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â 2.1.2) Lemmatization\n",
    "\n",
    "\n",
    "Lemmatization consist on properly use of a vocabulary and morphological analysis of words, aiming to remove inflectional endings only with the goal of returning any word to a set of base (or dictionary form) words.\n",
    "\n",
    "\n",
    "`Lemmatize(saw) = see`\n",
    "\n",
    "\n",
    "We will use a lemmatizer from WordNet (https://wordnet.princeton.edu) avaliable from nltk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"I was running and eating. This was a terrible idea.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "I                   I                   \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "This                This                \n",
      "was                 wa                  \n",
      "a                   a                   \n",
      "terrible            terrible            \n",
      "idea                idea                \n"
     ]
    }
   ],
   "source": [
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the words did no change!\n",
    "\n",
    "This is because there was no context. If we give a part of speech type then the lemmatizer will do what we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "I                   I                   \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "This                This                \n",
      "was                 be                  \n",
      "a                   a                   \n",
      "terrible            terrible            \n",
      "idea                idea                \n"
     ]
    }
   ],
   "source": [
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                PorterStemmer       LancasterStemmer    WordNetLemmatizer   \n",
      "\n",
      "dogs                dog                 dog                 dog                 \n",
      "destabilize         destabil            dest                destabilize         \n",
      "misunderstanding    misunderstand       misunderstand       misunderstanding    \n",
      "railroad            railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             football            \n",
      "pass                pass                pass                pas                 \n",
      "passing             pass                pass                passing             \n",
      "friendship          friendship          friend              friendship          \n",
      "friends             friend              friend              friend              \n",
      "friendships         friendship          friend              friendship          \n",
      "passed              pass                pass                passed              \n",
      "trouble             troubl              troubl              trouble             \n",
      "troubling           troubl              troubl              troubling           \n",
      "care                care                car                 care                \n",
      "believes            believ              believ              belief              \n"
     ]
    }
   ],
   "source": [
    "words = [\"dogs\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\n",
    "         \"football\",\"pass\",\"passing\",\"friendship\", \"friends\", \"friendships\",\n",
    "         \"passed\",\"trouble\",\"troubling\",\"care\", \"believes\"]\n",
    "preprocess = [porter, lancaster, wordnet_lemmatizer]\n",
    "\n",
    "len_bin = 20\n",
    "col_formater = \"{0:len_bin}{1:len_bin}{2:len_bin}{3:len_bin}\".replace(\"len_bin\",str(len_bin))\n",
    "print(col_formater.format(\"Word\", porter.__class__.__name__, lancaster.__class__.__name__, wordnet_lemmatizer.__class__.__name__))\n",
    "print(\"\")\n",
    "for w in words:\n",
    "    print( col_formater.format(w, porter.stem(w), lancaster.stem(w), wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2) Features for documents\n",
    "\n",
    "\n",
    "### 2.1) From docs to feature vectors: Make your own countvectorizer\n",
    "\n",
    "\n",
    "Let us build a simple document classifier featurizing each document by word counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.datasets\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = sklearn.datasets.fetch_20newsgroups()\n",
    "\n",
    "X_train = sklearn.datasets.fetch_20newsgroups(subset=\"train\").data\n",
    "y_train = sklearn.datasets.fetch_20newsgroups(subset=\"train\").target\n",
    "X_test  = sklearn.datasets.fetch_20newsgroups(subset=\"test\").data\n",
    "y_test  = sklearn.datasets.fetch_20newsgroups(subset=\"test\").target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X[\"data\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customising Vectoriser classes\n",
    "\n",
    "- **preprocessor**: a callable that takes an entire document as input (as a single string), and returns a possibly transformed version of the document, still as an entire string. This can be used to remove HTML tags, lowercase the entire document, etc.\n",
    "\n",
    "\n",
    "- **tokenizer**: a callable that takes the output from the preprocessor and splits it into tokens, then returns a list of these.\n",
    "\n",
    "\n",
    "- **analyzer**: a callable that replaces the preprocessor and tokenizer. The default analyzers all call the preprocessor and tokenizer, but custom analyzers will skip this. N-gram extraction and stop word filtering take place at the analyzer level, so a custom analyzer may have to reproduce these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Build a Simple countvectorizer\n",
    "\n",
    "Complete methods `fit` and `transform`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import re\n",
    "stemmer =  SnowballStemmer(language='english')\n",
    "\n",
    "retype = type(re.compile('hello, world'))\n",
    "\n",
    "class SimpleCountVectorizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, min_word_counts=1,\n",
    "                 tokenize_function=nltk.word_tokenize,\n",
    "                 dtype_featvec=np.int64,\n",
    "                 lemmatizer=None,\n",
    "                 stemmer=None,\n",
    "                 doc_cleaner=None):\n",
    "        \n",
    "        self.min_word_counts = min_word_counts\n",
    "        self.vocabulary = set()\n",
    "        self.word_to_ind = {}\n",
    "        self.tokenize = nltk.word_tokenize\n",
    "        self.dtype_featvec = dtype_featvec\n",
    "        self.lemmatizer = lemmatizer\n",
    "        self.stemmer = stemmer\n",
    "        self.doc_cleaner =  doc_cleaner\n",
    "\n",
    "    def transform_word(self,word):\n",
    "        word = word.lower()\n",
    "        if self.lemmatizer:\n",
    "            word = self.lemmatizer.lemmatize(word)\n",
    "        elif self.stemmer:\n",
    "            word = self.stemmer.stem(word)\n",
    "        return word\n",
    "    \n",
    "    def transform_doc(self, doc):\n",
    "        if isinstance(self.doc_cleaner,retype):\n",
    "            doc = self.doc_cleaner.sub(\" \", doc)\n",
    "        elif isinstance(self.doc_cleaner,str):\n",
    "            pattern = re.compile(self.doc_cleaner)\n",
    "            doc = pattern.sub(\" \", doc)\n",
    "            \n",
    "        return doc\n",
    "    \n",
    "    def fit(self, X):\n",
    "        #Start coding\n",
    "        assert self.vocabulary == set(), \"self.vocabulary is not empty it has {} words\".format(len(self.vocabulary))\n",
    "        assert isinstance(X,list), \"X is expected to be a list of documents\"\n",
    "        i = 0\n",
    "        for x in X:\n",
    "            x = self.transform_doc(x)\n",
    "            #Do something with the doc\n",
    "\n",
    "        # end coding\n",
    "        \n",
    "        self.n_features = len(self.vocabulary)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        #Start coding\n",
    "        encoded_X = scipy.sparse.lil_matrix()\n",
    "        for m, doc in enumerate(X):\n",
    "            doc = self.transform_doc(doc)\n",
    "            #Do something with the doc\n",
    "                    \n",
    "        # end coding\n",
    "        \n",
    "        return scipy.sparse.csr_matrix(encoded_X)\n",
    "        \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        encoded_X = self.transform(X)\n",
    "        return encoded_X\n",
    "    \n",
    "    def _words_in_vocab(self, X):\n",
    "        \n",
    "        if isinstance(X, str):\n",
    "            return [w for w in self.tokenize(X) if w in self.vocabulary]\n",
    "        \n",
    "        X_words_in_vocab = []\n",
    "        for sentence in X:\n",
    "            X_words_in_vocab.append(self.tokenize(sentence))\n",
    "            \n",
    "        return X_words_in_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer = SimpleCountVectorizer(lemmatizer= None, \n",
    "                                                #stemmer= SnowballStemmer('english')\n",
    "                                                stemmer=None \n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 180 ms, sys: 1.53 ms, total: 182 ms\n",
      "Wall time: 181 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleCountVectorizer(doc_cleaner=None, dtype_featvec=<class 'numpy.int64'>,\n",
       "           lemmatizer=None, min_word_counts=1, stemmer=None,\n",
       "           tokenize_function=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "simple_count_vectorizer.fit(X_train[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6904"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple_count_vectorizer.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x6904 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [\"this is a feature vector for this sentence\"]\n",
    "x = simple_count_vectorizer.transform(A)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x6904 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [\"this is a feature vector for this sentence\",\"This is another sentence\"]\n",
    "x = simple_count_vectorizer.transform(A)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'feature', 'vector', 'for', 'this', 'sentence']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_count_vectorizer.tokenize(A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'feature', 'for', 'this']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_vocab = [w for w in simple_count_vectorizer.tokenize(A[0]) if w in simple_count_vectorizer.vocabulary]\n",
    "words_in_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_in_vocab), len(set(words_in_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training a document classifier with `SimpleCountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I) No Stemmer and no doc_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vainilla_count_vectorizer = SimpleCountVectorizer(lemmatizer= None, \n",
    "                                                stemmer= None,\n",
    "                                                doc_cleaner=None)\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_0 = sklearn.pipeline.Pipeline([(\"countvectorizer\", vainilla_count_vectorizer),\n",
    "                                         (\"logisticregression\", logistic)],\n",
    "                                         )#memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9836485769842673    Accuracy test: 0.7749601699415826\n",
      "CPU times: user 3min 42s, sys: 688 ms, total: 3min 43s\n",
      "Wall time: 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_0.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_0.predict(X_test)\n",
    "y_train_pred = model_pipe_0.predict(X_train)\n",
    "\n",
    "acc_train_0 = np.mean(y_train == y_train_pred)\n",
    "acc_test_0 = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_0, acc_test_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x187171 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 98 stored elements in Compressed Sparse Row format>, 187171)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_0.steps[0][1].transform(X_train[0:1]), model_pipe_0.steps[0][1].n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II) No stemmer but doc_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer = SimpleCountVectorizer(lemmatizer= None, \n",
    "                                                stemmer= None,\n",
    "                                                doc_cleaner=re.compile(\"[^a-zA-Z]\"))\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_1 = sklearn.pipeline.Pipeline([(\"countvectorizer\", simple_count_vectorizer),\n",
    "                                        (\"logisticregression\", logistic)],\n",
    "                                         )#memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 15s, sys: 344 ms, total: 1min 15s\n",
      "Wall time: 1min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', SimpleCountVectorizer(doc_cleaner=re.compile('[^a-zA-Z]'),\n",
       "           dtype_featvec=<class 'numpy.int64'>, lemmatizer=None,\n",
       "           min_word_counts=1, stemmer=None, tokenize_function=None)), ('logisticregression', LogisticRegression(C=0.1, class_weight=None, dual=False,...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9961993989747215    Accuracy test: 0.804567180031864\n"
     ]
    }
   ],
   "source": [
    "y_test_pred  = model_pipe_1.predict(X_test)\n",
    "y_train_pred = model_pipe_1.predict(X_train)\n",
    "\n",
    "acc_train_1 = np.mean(y_train == y_train_pred)\n",
    "acc_test_1 = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_1, acc_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x89033 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 90 stored elements in Compressed Sparse Row format>, 89033)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_1.steps[0][1].transform(X_train[0:1]), model_pipe_1.steps[0][1].n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III) Use a SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer_stemmer = SimpleCountVectorizer(lemmatizer= None,\n",
    "                                                        stemmer= SnowballStemmer('english'),\n",
    "                                                        doc_cleaner=re.compile(\"[^a-zA-Z]\"))\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_2 = sklearn.pipeline.Pipeline([(\"countvectorizer\", simple_count_vectorizer_stemmer),\n",
    "                                        (\"logisticregression\", logistic)],\n",
    "                                         )#memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9969064875375641    Accuracy test: 0.8081518852894317\n",
      "CPU times: user 3min 57s, sys: 1.1 s, total: 3min 58s\n",
      "Wall time: 3min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_2.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_2.predict(X_test)\n",
    "y_train_pred = model_pipe_2.predict(X_train)\n",
    "\n",
    "acc_train_2 = np.mean(y_train == y_train_pred)\n",
    "acc_test_2  = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_2, acc_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<1x69983 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 88 stored elements in Compressed Sparse Row format>, 69983)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_2.steps[0][1].transform(X_train[0:1]), model_pipe_2.steps[0][1].n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table with results for each pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results[\"no clean no stem\"]   = [acc_train_0, acc_test_0]\n",
    "df_results[\"yes clean no stem\"]  = [acc_train_1, acc_test_1]\n",
    "df_results[\"yes clean yes stem\"] = [acc_train_2, acc_test_2]\n",
    "df_results.index=[\"train\",\"test\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no clean no stem</th>\n",
       "      <th>yes clean no stem</th>\n",
       "      <th>yes clean yes stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.996199</td>\n",
       "      <td>0.996906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.774960</td>\n",
       "      <td>0.804567</td>\n",
       "      <td>0.808152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no clean no stem  yes clean no stem  yes clean yes stem\n",
       "train          0.983649           0.996199            0.996906\n",
       "test           0.774960           0.804567            0.808152"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2)  Ngram features with Sklearn vectorizer\n",
    "\n",
    "\n",
    "#### IV) Training a document classifier with sklearn `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_3 = sklearn.pipeline.Pipeline([(\"countvectorizer\", count_vectorizer),\n",
    "                                        (\"logisticregression\", logistic)],\n",
    "                                        )# memory='/Users/Shared/sklearn_mem/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9976135761004066    Accuracy test: 0.8070897503983006\n",
      "CPU times: user 30.5 s, sys: 197 ms, total: 30.7 s\n",
      "Wall time: 30.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_3.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_3.predict(X_test)\n",
    "y_train_pred = model_pipe_3.predict(X_train)\n",
    "\n",
    "acc_train_3 = np.mean(y_train == y_train_pred)\n",
    "acc_test_3  = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_3, acc_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_3.steps[0][1].transform(X_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results[\"sklearn countvectorizer\"] = [acc_train_3, acc_test_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no clean no stem</th>\n",
       "      <th>yes clean no stem</th>\n",
       "      <th>yes clean yes stem</th>\n",
       "      <th>sklearn countvectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.996199</td>\n",
       "      <td>0.996906</td>\n",
       "      <td>0.997614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.774960</td>\n",
       "      <td>0.804567</td>\n",
       "      <td>0.808152</td>\n",
       "      <td>0.807090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no clean no stem  yes clean no stem  yes clean yes stem  \\\n",
       "train          0.983649           0.996199            0.996906   \n",
       "test           0.774960           0.804567            0.808152   \n",
       "\n",
       "       sklearn countvectorizer  \n",
       "train                 0.997614  \n",
       "test                  0.807090  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V) Training a document classifier with sklearn `CountVectorizer` and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2))\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "\n",
    "model_pipe_4 = sklearn.pipeline.Pipeline([(\"countvectorizer\", count_vectorizer),\n",
    "                                        (\"logisticregression\", logistic)],\n",
    "                                        )# memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.9994696835778681    Accuracy test: 0.8104089219330854\n",
      "CPU times: user 2min 2s, sys: 755 ms, total: 2min 2s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_4.fit(X_train,y_train)\n",
    "\n",
    "y_test_pred  = model_pipe_4.predict(X_test)\n",
    "y_train_pred = model_pipe_4.predict(X_train)\n",
    "\n",
    "acc_train_4 = np.mean(y_train == y_train_pred)\n",
    "acc_test_4  = np.mean(y_test == y_test_pred)\n",
    "\n",
    "print(\"Accuracy train: {}    Accuracy test: {}\".format(acc_train_4, acc_test_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1181803 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 202 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe_4.steps[0][1].transform(X_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results[\"sklearn countvectorizer 2gram\"] = [acc_train_4, acc_test_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no clean no stem</th>\n",
       "      <th>yes clean no stem</th>\n",
       "      <th>yes clean yes stem</th>\n",
       "      <th>sklearn countvectorizer</th>\n",
       "      <th>sklearn countvectorizer 2gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.996199</td>\n",
       "      <td>0.996906</td>\n",
       "      <td>0.997614</td>\n",
       "      <td>0.999470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.774960</td>\n",
       "      <td>0.804567</td>\n",
       "      <td>0.808152</td>\n",
       "      <td>0.807090</td>\n",
       "      <td>0.810409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no clean no stem  yes clean no stem  yes clean yes stem  \\\n",
       "train          0.983649           0.996199            0.996906   \n",
       "test           0.774960           0.804567            0.808152   \n",
       "\n",
       "       sklearn countvectorizer  sklearn countvectorizer 2gram  \n",
       "train                 0.997614                       0.999470  \n",
       "test                  0.807090                       0.810409  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1b80a278>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2b595b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_results.plot(kind=\"bar\", ylim=(0.8,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.3) Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1) SelectKbest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_selector = SelectKBest(chi2, k = 700000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_vectors = model_pipe_4.steps[0][1].transform(X_train)\n",
    "X_test_vectors  = model_pipe_4.steps[0][1].transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1181803)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=700000, score_func=<function chi2 at 0x10eeb67b8>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selector.fit(X_train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_vectors_selected = feature_selector.transform(X_train_vectors)\n",
    "X_test_vectors_selected  = feature_selector.transform(X_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 700000)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(X_train_vectors_selected,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_train = np.mean(logistic.predict(X_train_vectors_selected) == y_train)\n",
    "acc_test = np.mean(logistic.predict(X_test_vectors_selected) == y_test)\n",
    "df_results[\"sklearn countvectorizer 2gram + selection\"] = [acc_train, acc_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no clean no stem</th>\n",
       "      <th>yes clean no stem</th>\n",
       "      <th>yes clean yes stem</th>\n",
       "      <th>sklearn countvectorizer</th>\n",
       "      <th>sklearn countvectorizer 2gram</th>\n",
       "      <th>sklearn countvectorizer 2gram + selection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.996199</td>\n",
       "      <td>0.996906</td>\n",
       "      <td>0.997614</td>\n",
       "      <td>0.999470</td>\n",
       "      <td>0.999470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.774960</td>\n",
       "      <td>0.804567</td>\n",
       "      <td>0.808152</td>\n",
       "      <td>0.807090</td>\n",
       "      <td>0.810409</td>\n",
       "      <td>0.811073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no clean no stem  yes clean no stem  yes clean yes stem  \\\n",
       "train          0.983649           0.996199            0.996906   \n",
       "test           0.774960           0.804567            0.808152   \n",
       "\n",
       "       sklearn countvectorizer  sklearn countvectorizer 2gram  \\\n",
       "train                 0.997614                       0.999470   \n",
       "test                  0.807090                       0.810409   \n",
       "\n",
       "       sklearn countvectorizer 2gram + selection  \n",
       "train                                   0.999470  \n",
       "test                                    0.811073  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2) Select K for SelectKbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = X_train_vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_k = n_features//2\n",
    "max_k = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_selection.univariate_selection.SelectKBest"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "possible_K = [int(x) for x in np.linspace(min_k, max_k,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 590901,  Accuracy train: 0.9994696835778681    Accuracy test: 0.8108072225172597\n",
      "K = 656556,  Accuracy train: 0.9994696835778681    Accuracy test: 0.8105416887944769\n",
      "K = 722212,  Accuracy train: 0.9994696835778681    Accuracy test: 0.8112055231014339\n",
      "K = 787868,  Accuracy train: 0.9994696835778681    Accuracy test: 0.8113382899628253\n",
      "K = 853524,  Accuracy train: 0.9994696835778681    Accuracy test: 0.8114710568242167\n",
      "K = 919179,  Accuracy train: 0.9994696835778681    Accuracy test: 0.8106744556558683\n",
      "K = 984835,  Accuracy train: 0.9994696835778681    Accuracy test: 0.8109399893786511\n",
      "K = 1050491,  Accuracy train: 0.9994696835778681    Accuracy test: 0.8108072225172597\n",
      "K = 1116147,  Accuracy train: 0.9994696835778681    Accuracy test: 0.8113382899628253\n",
      "K = 1181803,  Accuracy train: 0.9994696835778681    Accuracy test: 0.8104089219330854\n",
      "CPU times: user 13min 56s, sys: 4.97 s, total: 14min 1s\n",
      "Wall time: 14min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "for k in possible_K:\n",
    "    \n",
    "    feature_selector = SelectKBest(chi2, k = k)\n",
    "    feature_selector.fit(X_train_vectors, y_train)\n",
    "    X_train_vectors_selected = feature_selector.transform(X_train_vectors)\n",
    "    X_test_vectors_selected  = feature_selector.transform(X_test_vectors)\n",
    "    \n",
    "    logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "    logistic.fit(X_train_vectors_selected,y_train)\n",
    "    acc_train = np.mean(logistic.predict(X_train_vectors_selected) == y_train)\n",
    "    acc_test = np.mean(logistic.predict(X_test_vectors_selected) == y_test)\n",
    "    print(\"K = {},  Accuracy train: {}    Accuracy test: {}\".format(k,acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3) Feature Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer_stemmer = SimpleCountVectorizer(lemmatizer= None,\n",
    "                                                        stemmer= SnowballStemmer('english'),\n",
    "                                                        doc_cleaner=re.compile(\"[^a-zA-Z]\"))\n",
    "\n",
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "union = sklearn.pipeline.FeatureUnion([(\"simple_count_vectorizer_stemmer\", simple_count_vectorizer_stemmer),\n",
    "                                       (\"count_vectorizer\", count_vectorizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[^a-zA-Z]', re.UNICODE)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union.transformer_list[0][1].doc_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "feature_selector = SelectKBest(chi2, k = 700000)\n",
    "model_pipe_5 = sklearn.pipeline.Pipeline([(\"union_vectorizers\", union),\n",
    "                                          (\"feature_selector\", feature_selector),\n",
    "                                          (\"logisticregression\", logistic)],\n",
    "                                         )# memory='/Users/Shared/sklearn_mem/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 53s, sys: 1.24 s, total: 3min 54s\n",
      "Wall time: 3min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('union_vectorizers', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('simple_count_vectorizer_stemmer', SimpleCountVectorizer(doc_cleaner=re.compile('[^a-zA-Z]'),\n",
       "           dtype_featvec=<class 'numpy.int64'>, lemmatizer=None,\n",
       "           min_word_counts=1,\n",
       "           stemmer=<nltk.stem.sno...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_pipe_5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_train = np.mean(model_pipe_5.predict(X_train) == y_train)\n",
    "acc_test = np.mean(model_pipe_5.predict(X_test) == y_test)\n",
    "df_results[\"Feature union + selection\"] = [acc_train, acc_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no clean no stem</th>\n",
       "      <th>yes clean no stem</th>\n",
       "      <th>yes clean yes stem</th>\n",
       "      <th>sklearn countvectorizer</th>\n",
       "      <th>sklearn countvectorizer 2gram</th>\n",
       "      <th>sklearn countvectorizer 2gram + selection</th>\n",
       "      <th>Feature union + selection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.996199</td>\n",
       "      <td>0.996906</td>\n",
       "      <td>0.997614</td>\n",
       "      <td>0.999470</td>\n",
       "      <td>0.999470</td>\n",
       "      <td>0.999646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.774960</td>\n",
       "      <td>0.804567</td>\n",
       "      <td>0.808152</td>\n",
       "      <td>0.807090</td>\n",
       "      <td>0.810409</td>\n",
       "      <td>0.811073</td>\n",
       "      <td>0.819570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       no clean no stem  yes clean no stem  yes clean yes stem  \\\n",
       "train          0.983649           0.996199            0.996906   \n",
       "test           0.774960           0.804567            0.808152   \n",
       "\n",
       "       sklearn countvectorizer  sklearn countvectorizer 2gram  \\\n",
       "train                 0.997614                       0.999470   \n",
       "test                  0.807090                       0.810409   \n",
       "\n",
       "       sklearn countvectorizer 2gram + selection  Feature union + selection  \n",
       "train                                   0.999470                   0.999646  \n",
       "test                                    0.811073                   0.819570  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4) Crossvalidating results (Exercise)\n",
    "\n",
    "It is very (VERY) important you do crossvalidation. \n",
    "\n",
    "\n",
    "If you have never use Bayesian Optimization for selecting hyperparameters I recommend you to try: https://scikit-optimize.github.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can't deepcopy objects with compiled regular expressions\n",
    "# Therefore can't use GridSearch with our doc_cleaner with a compiled regular expression\n",
    "# Minimal working example\n",
    "#import re,copy\n",
    "#class MyClass():\n",
    "#    def __init__(self):\n",
    "#        self.regex=re.compile('\\d+')\n",
    "#\n",
    "#myobj = MyClass()    \n",
    "#copy.deepcopy(myobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train_vec = union.fit_transform(X_train)\n",
    "#n_features = X_train_vec.shape[1]\n",
    "#min_k = n_features//2\n",
    "#max_k = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_count_vectorizer_stemmer = SimpleCountVectorizer(lemmatizer= None,\n",
    "                                                        stemmer= SnowballStemmer('english'),\n",
    "                                                        doc_cleaner=r\"[^a-zA-Z]\")\n",
    "\n",
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "union = sklearn.pipeline.FeatureUnion([(\"simple_count_vectorizer_stemmer\", simple_count_vectorizer_stemmer),\n",
    "                                       (\"count_vectorizer\", count_vectorizer)])\n",
    "\n",
    "logistic = sklearn.linear_model.LogisticRegression(C=0.1)\n",
    "feature_selector = SelectKBest(chi2)\n",
    "\n",
    "\n",
    "# Hint\n",
    "#model_pipe_6 = sklearn.pipeline.Pipeline([(\"union_vectorizers\", union),\n",
    "#                                          (\"feature_selector\", feature_selector),\n",
    "#                                          (\"logisticregression\", logistic)],\n",
    "#                                         )# memory='/Users/Shared/sklearn_mem/')\n",
    "\n",
    "#possible_K = [int(x)-1 for x in np.linspace(min_k, max_k,10)]\n",
    "#parameteres = {'feature_selector__k':possible_K, 'logisticregression__C':[0.1,0.01,0.001]}\n",
    "\n",
    "#grid_6 = sklearn.model_selection.RandomizedSearchCV(model_pipe_6,\n",
    "#                                                    param_distributions=parameteres, \n",
    "#                                                    cv=3,\n",
    "#                                                    n_iter=2,\n",
    "#                                                    n_jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%time\n",
    "#grid_6.fit(X_train, y_train)\n",
    "# Returns error? Welcome to your custom sklearn pipelines\n",
    "# Question: Why is there an error?\n",
    "# ValueError: k should be >=0, <= n_features; got 3238254.Use k='all' to return all features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#acc_train = np.mean(grid_6.predict(X_train) == y_train)\n",
    "#acc_test = np.mean(grid_6.predict(X_test) == y_test)\n",
    "#print(\"acc_train={} acc_test={}\".format(acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_results[\"Feature union + selection + CV\"] = [acc_train, acc_test]\n",
    "#df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Hashing words (Exercise)\n",
    "\n",
    "Explore the results you get by hashing the input words. Add `df_results[\"Feature hash + selection + CV\"]` in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
